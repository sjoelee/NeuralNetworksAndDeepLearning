{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10]) #where does 784 come from??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.SGD(training_data=training_data, epochs=30, mini_batch_size=10, eta=3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "# needed to make sure we always get the same set of randomized weights when we begin\n",
    "np.random.seed(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1. + np.exp(-z))\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(x, y):\n",
    "    # use squared error\n",
    "    return 0.5 * (np.linalg.norm(x-y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = [784, 30, 10]\n",
    "\n",
    "x = training_data[0][0]\n",
    "y = training_data[0][1]\n",
    "\n",
    "num_layers = len(sizes)\n",
    "biases = [np.random.randn(s, 1) for s in sizes[1:]]\n",
    "weights = [np.random.randn(s1, s2) for s1, s2 in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_weights = deepcopy(weights)\n",
    "old_biases = deepcopy(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = deepcopy(old_weights)\n",
    "biases = deepcopy(old_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feedforward\n",
    "# initialize\n",
    "activations = [x]\n",
    "z_inputs = [x]\n",
    "z = np.dot(weights[0].T, x)\n",
    "for w, b in zip(weights, biases):\n",
    "    z = np.dot(w.T, activations[-1]) + b\n",
    "    z_inputs.append(z)\n",
    "    a = sigmoid(z)\n",
    "    activations.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backpropagate\n",
    "# for layer L\n",
    "# assume we have more than one layer...\n",
    "eta = 0.3\n",
    "nabla_b = [np.zeros((s, 1)) for s in sizes[1:]]\n",
    "nabla_w = [np.zeros((s1, s2)) for s1, s2 in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "#initialize (for layer L = 2)\n",
    "# note that the length of activations and z_inputs are different from nabla_b and nabla_w\n",
    "# because activations and z_inputs includes the input\n",
    "delta = (activations[-1] - y) * sigmoid_prime(z_inputs[-1])\n",
    "nabla_b[-1] = delta\n",
    "nabla_w[-1] = np.dot(activations[-2], delta.T)\n",
    "biases[-1] = biases[-1] - eta * nabla_b[-1]\n",
    "weights[-1] = weights[-1] - eta * nabla_w[-1]\n",
    "\n",
    "# for all previous layers\n",
    "for l in range(num_layers-1, 1, -1):\n",
    "    delta = np.dot(weights[l-1], delta) * sigmoid_prime(z_inputs[l-1])\n",
    "    nabla_b[l-2] = delta\n",
    "    nabla_w[l-2] = np.dot(activations[l-2], delta.T)\n",
    "    biases[l-2] = biases[l-2] - eta * nabla_b[l-2]\n",
    "    weights[l-2] = weights[l-2] - eta * nabla_w[l-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nabla_b_old = deepcopy(nabla_b)\n",
    "nabla_w_old = deepcopy(nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine whether or not the gradient is being calculated correctly by comparing our nabla_b and nabla_w matrices to another calculation of that gradient. To do so, we calculate: dC/dw_1_ij = (cost(weights[1][i][j] + epsilon) - cost(weights[1][i][j])) / epsilon. Then we compare that value to nabla_w[1][i][j]. \n",
    "\n",
    "We can do this for all the entries in each weight matrix. I did this for a few random entries and spot-checking it that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "n = len(data)\n",
    "# batches = [data[i*batch_size:i*batch_size+batch_size] for i in xrange(0, n, batch_size)]\n",
    "batches = [data[i:i+batch_size] for i in xrange(0, n, batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = training_data\n",
    "batch_size = 1\n",
    "n = len(data)\n",
    "batch_data = [data[i:i+batch_size] for i in xrange(0, n, batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[100][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.biases = [np.random.randn(s, 1) for s in sizes[1:]]\n",
    "        self.weights = [np.random.randn(s1, s2) for s1, s2 in zip(sizes[:-1], sizes[1:])]\n",
    "        self.activations = []\n",
    "        self.z_inputs = []\n",
    "        \n",
    "        np.random.seed(2015)\n",
    "    \n",
    "    def train(self, data, num_epochs=1, batch_size=1):\n",
    "        for idx_epoch in range(num_epochs):\n",
    "            n = len(data)\n",
    "            batch_data = [data[i:i+batch_size] for i in xrange(0, n, batch_size)]\n",
    "            for idx, batch in enumerate(batch_data):\n",
    "                self.feedforward(x=batch[0][0], y=batch[0][1]) ## need to address this for the batch size\n",
    "                self.backpropagate(y=batch[0][1])\n",
    "\n",
    "    def test(self, data):\n",
    "        error_rate = 0\n",
    "        for test_data in data:\n",
    "            x_test = test_data[0]\n",
    "            y_test = test_data[1]\n",
    "            self.feedforward(x_test, y_test)\n",
    "            error = (y_test != self.activations[-1])\n",
    "            if error:\n",
    "                num_error += 1\n",
    "        \n",
    "        return num_error / (1.*len(data))\n",
    "\n",
    "    def feedforward(self, x, y):\n",
    "        self.activations = [x]\n",
    "        self.z_inputs = [x]\n",
    "\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w.T, self.activations[-1]) + b\n",
    "            self.z_inputs.append(z)\n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "    \n",
    "    def backpropagate(self, y, eta=0.3):\n",
    "        # assume we have more than one layer...\n",
    "        nabla_b = [np.zeros((s, 1)) for s in self.sizes[1:]]\n",
    "        nabla_w = [np.zeros((s1, s2)) for s1, s2 in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "        # for layer L\n",
    "        # note that the length of activations and z_inputs are different from nabla_b and nabla_w\n",
    "        # because activations and z_inputs includes the input in the first entry\n",
    "        delta = (self.activations[-1] - y) * sigmoid_prime(self.z_inputs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(self.activations[-2], delta.T)\n",
    "        self.biases[-1] = self.biases[-1] - eta * nabla_b[-1]\n",
    "        self.weights[-1] = self.weights[-1] - eta * nabla_w[-1]\n",
    "\n",
    "        # for all previous layers\n",
    "        for l in range(num_layers-1, 1, -1):\n",
    "            delta = np.dot(self.weights[l-1], delta) * sigmoid_prime(self.z_inputs[l-1])\n",
    "            nabla_b[l-2] = delta\n",
    "            nabla_w[l-2] = np.dot(self.activations[l-2], delta.T)\n",
    "            self.biases[l-2] = self.biases[l-2] - eta * nabla_b[l-2]\n",
    "            self.weights[l-2] = self.weights[l-2] - eta * nabla_w[l-2]\n",
    "                \n",
    "#     def update_mini_batch(self, mini_batch, eta):\n",
    "#         batch_size = len(mini_batch)\n",
    "#         nabla_b = [np.zeros((s, 1)) for s in self.sizes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = [784, 30, 10]\n",
    "nn = NeuralNetwork(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[10000][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.train(training_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-62f5623e92f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-f56307bf6d51>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mnum_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "nn.test(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
